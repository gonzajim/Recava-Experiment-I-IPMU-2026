# -*- coding: utf-8 -*-
"""Experimento_IPMU2026_Iterative Performance Analysis: 25-Question Experiment Results.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i2CUB-_EVYJDFki6F2NZMPuJXkVNrYLH
"""

# -*- coding: utf-8 -*-
"""Experimento_RECAV_AI_OPTIMIZADO_FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Ib05zp9zzxR6rZbn58PVY6aLWLEFrJz

# Experimento RECAV-AI Fase II - Paper IPMU 2026
## Optimizaci√≥n de Calidad Legal en Sistemas RAG mediante Refinamiento Adversarial por Agregaci√≥n Fuzzy

**Autores:** Gonzalo Jimenez, Andres Montoro Montarroso, Jose Angel Olivas Varela
**Fecha:** Febrero 2026
"""

# @title Mount Google Drive and Install Dependencies
from google.colab import drive, userdata
import os
import shutil
import sys
from datetime import datetime

# Mount Google Drive
drive.mount('/content/drive')

# Install required libraries
!pip install openai pinecone sentence-transformers pandas tabulate -q

# Configuration of paths
PATH_RECAVA = "/content/drive/MyDrive/RECAVA-AI-STARTUP/Experimentos"
os.makedirs(PATH_RECAVA, exist_ok=True)

print("‚úÖ Environment ready.")

# @title Knowledge Base Integration
from pinecone import Pinecone
from sentence_transformers import SentenceTransformer
from functools import lru_cache

# Retrieve API Keys securely
PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')
OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')

# Initialize Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)
# Replace with your specific index host
index = pc.Index(host="https://uclm-corpus-roma-dptaw1c.svc.aped-4627-b74a.pinecone.io")

# Initialize Embeddings model
embed_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

def hybrid_search_engine(user_query, top_k=10):
    """Retrieve context from Pinecone using hybrid search."""
    query_emb = embed_model.encode(user_query).tolist()
    results = index.query(
        vector=query_emb,
        top_k=top_k,
        include_metadata=True
    )
    context = "\n\n".join([m['metadata'].get('text', '') for m in results['matches']])
    return context[:4000]  # Limit context size


# ‚ö° OPTIMIZACI√ìN: Versi√≥n cacheada para evitar b√∫squedas repetidas
@lru_cache(maxsize=100)
def hybrid_search_engine_cached(user_query, top_k=10):
    """
    Versi√≥n cacheada de hybrid_search_engine.
    Evita b√∫squedas repetidas, ahorrando ~1-2 seg por query.
    Cache: 100 queries m√°ximo.
    """
    return hybrid_search_engine(user_query, top_k)

print("‚úÖ Knowledge base connected.")
print("‚ö° Cache habilitado para b√∫squedas (100 queries m√°x).")

# @title Local Inference Server Configuration
import openai

# Replace with your actual Ngrok URL
URL_NGROK = "https://prodigally-nonaddicting-deborah.ngrok-free.dev/v1"

client_local = openai.OpenAI(base_url=URL_NGROK, api_key="lm-studio")
client_cloud = openai.OpenAI(api_key=OPENAI_API_KEY)

# Define the models to be used in the swarm
LOCAL_MODELS = [
    "mistral_7b_0-3_oh-dcft-v3.1-claude-3-5-sonnet-20241022",
    "google/gemma-3-4b",
    "deepseek/deepseek-r1-distill-llama-8b"
]

print("‚úÖ Models configured.")

# @title Behavioral Configuration & Rationale

# Generator Behavior: Se a√±ade el placeholder para el RL num√©rico
MAIN_MODEL_BEHAVIOR = """
Eres un experto senior en sostenibilidad, diligencia debida y cumplimiento normativo.
Tu objetivo es generar una respuesta de m√°xima excelencia t√©cnica.
Usa lenguaje del dominio legal; cita art√≠culos y est√°ndares espec√≠ficos (ISO, ESRS, GRI).

{feedback_placeholder}
"""

# Auditor Behavior: Escala 0-10 estricta
AUDITOR_BEHAVIOR = """
Eres un auditor t√©cnico EXTREMADAMENTE ESTRICTO que eval√∫a en escala 0-10 (0=nulo, 10=perfecto).

TU MISI√ìN: Evaluar objetivamente si la respuesta cumple con el criterio preguntado.

ESCALA DE PUNTUACI√ìN:
- 0-2: No cumple en absoluto con el criterio
- 3-4: Cumplimiento muy deficiente, carencias graves
- 5-6: Cumplimiento parcial, con carencias importantes
- 7-8: Cumple adecuadamente pero con mejoras necesarias
- 9-10: Cumplimiento excelente y completo del criterio

S√â EXTREMADAMENTE CR√çTICO Y EXIGENTE:
- Si falta informaci√≥n clave ‚Üí penaliza duramente
- Si hay vaguedades o generalidades ‚Üí penaliza
- Si no hay citas normativas concretas (art√≠culos, est√°ndares ISO, directivas espec√≠ficas) ‚Üí penaliza
- Si hay informaci√≥n incorrecta o hallucinations ‚Üí penaliza severamente
- Si no responde directamente a la pregunta del criterio ‚Üí penaliza

S√â HONESTO: Si la respuesta no cumple el criterio, dilo claramente con nota baja.
NO seas indulgente. La excelencia debe ganarse.

Te pedir√°n que eval√∫es un subcriterio respondiendo a una pregunta espec√≠fica.
Eval√∫a SOLO en base a si responde bien a esa pregunta.
"""

# Contrato JSON esperado de cada auditor
JSON_CONTRACT = """
INSTRUCCIONES DE EVALUACI√ìN:
1. Lee la respuesta completa detenidamente
2. Responde honestamente: ¬øLa respuesta cumple con lo que pregunta el criterio?
3. Identifica carencias espec√≠ficas y concretas
4. Asigna nota objetiva seg√∫n el grado de cumplimiento (0-10)

DEVUELVE √öNICAMENTE este formato JSON exacto:
{
  "nota": float (0.0 a 10.0, eval√∫a objetivamente el cumplimiento),
  "comentario": str (m√°ximo 50 palabras, menciona carencias espec√≠ficas encontradas)
}

S√© estricto pero justo. Eval√∫a solo lo que se pregunta.
"""

print("‚úÖ Behavioral prompts configured.")

# @title RECAVA Quality Protocol (25 Sub-Criteria) - PESOS DEL DOCUMENTO OFICIAL

CRITERIOS_RECAVA = {
    "C1": {
        "nombre": "Estructura Clara y Jerarquizada de la Informaci√≥n",
        "peso": 0.7,  # Alta
        "subcriterios": {
            "C1.1": {
                "desc": "Dividir en Secciones Claras (T√≠tulos/Subt√≠tulos)",
                "pregunta": "¬øLa respuesta est√° dividida en secciones con t√≠tulos o subt√≠tulos claros?"
            },
            "C1.2": {
                "desc": "Priorizar Informaci√≥n seg√∫n Relevancia",
                "pregunta": "¬øExiste una progresi√≥n l√≥gica de la informaci√≥n, de lo m√°s general a lo m√°s espec√≠fico?"
            },
            "C1.3": {
                "desc": "Incluir Ejemplos Ilustrativos",
                "pregunta": "¬øSe resaltan las ideas m√°s importantes y se prioriza la informaci√≥n relevante para el usuario?"
            },
            "C1.4": {
                "desc": "Proporcionar Introducci√≥n/Resumen Inicial",
                "pregunta": "¬øSe emplean elementos visuales (listas, vi√±etas, destacados) que faciliten la lectura?"
            },
            "C1.5": {
                "desc": "Mantener Formato Coherente",
                "pregunta": "¬øSe incluyen ejemplos concretos que ayuden a entender mejor los conceptos presentados?"
            }
        },
        "pesos_sub": [0.9, 0.7, 0.5, 0.3, 0.3]  # Suma: 2.7
    },

    "C2": {
        "nombre": "Precisi√≥n, Pertinencia y Adecuaci√≥n Contextual",
        "peso": 0.9,  # Muy alta
        "subcriterios": {
            "C2.1": {
                "desc": "Adaptar Lenguaje y Nivel de Detalle",
                "pregunta": "¬øLa respuesta se ajusta al tema y al nivel de detalle de la pregunta realizada?"
            },
            "C2.2": {
                "desc": "Incluir Informaci√≥n Esencial sin Tecnicismos Excesivos",
                "pregunta": "¬øLa informaci√≥n sobre normativas (cuando aplica) es exacta y fiel a la fuente consultada?"
            },
            "C2.3": {
                "desc": "Contextualizar seg√∫n Sector/Tama√±o Empresa",
                "pregunta": "¬øLa respuesta evita informaci√≥n irrelevante o demasiado gen√©rica para el contexto planteado?"
            },
            "C2.4": {
                "desc": "Ser Fiel a la Normativa Solicitada (sin mezclar)",
                "pregunta": "¬øSe ofrecen ejemplos o referencias aplicables a distintos sectores o realidades (cuando corresponde)?"
            },
            "C2.5": {
                "desc": "Evitar Inconsistencias o Contradicciones",
                "pregunta": "¬øEl lenguaje y la profundidad t√©cnica de la respuesta se adecuan al tipo de usuario o escenario espec√≠fico?"
            }
        },
        "pesos_sub": [0.3, 0.3, 0.7, 0.9, 0.9]  # Suma: 3.1
    },

    "C3": {
        "nombre": "Enfoque Pr√°ctico y Orientaci√≥n a la Decisi√≥n",
        "peso": 0.5,  # Medio
        "subcriterios": {
            "C3.1": {
                "desc": "Incluir Recomendaciones Concretas",
                "pregunta": "¬øLa respuesta ofrece sugerencias espec√≠ficas para aplicar la informaci√≥n o normativa?"
            },
            "C3.2": {
                "desc": "Destacar Ventajas y Desventajas",
                "pregunta": "¬øSe mencionan ventajas y desventajas de las distintas opciones propuestas?"
            },
            "C3.3": {
                "desc": "Proporcionar Pasos Concretos de Implementaci√≥n",
                "pregunta": "¬øDescribe pasos o una secuencia l√≥gica para la implementaci√≥n?"
            },
            "C3.4": {
                "desc": "Sugerir Indicadores o M√©tricas de Evaluaci√≥n",
                "pregunta": "¬øAvisa sobre posibles obst√°culos o limitaciones relevantes para la toma de decisiones?"
            },
            "C3.5": {
                "desc": "Resaltar Mejores Pr√°cticas o Estrategias Comprobadas",
                "pregunta": "¬øIncluye recomendaciones sobre cu√°ndo buscar ayuda externa o asesor√≠a especializada?"
            }
        },
        "pesos_sub": [0.7, 0.3, 0.9, 0.7, 0.1]  # Suma: 2.7
    },

    "C4": {
        "nombre": "Inclusi√≥n de Referencias Esenciales y Recursos Adicionales",
        "peso": 0.5,  # Media
        "subcriterios": {
            "C4.1": {
                "desc": "Proporcionar Enlaces a Fuentes Oficiales",
                "pregunta": "¬øSe citan fuentes u organizaciones reconocidas que respalden la informaci√≥n?"
            },
            "C4.2": {
                "desc": "Mencionar Marcos Legales o Est√°ndares Internacionales",
                "pregunta": "¬øSe ofrecen enlaces a gu√≠as, herramientas o portales oficiales para ampliar la informaci√≥n?"
            },
            "C4.3": {
                "desc": "Incorporar Gu√≠as o Metodolog√≠as Espec√≠ficas",
                "pregunta": "¬øSe mencionan normativas o est√°ndares internacionales pertinentes al tema?"
            },
            "C4.4": {
                "desc": "Conectar con Instituciones o Expertos Reconocidos",
                "pregunta": "¬øSe hace referencia a manuales t√©cnicos o metodolog√≠as espec√≠ficas (cuando aplica)?"
            },
            "C4.5": {
                "desc": "Asegurar Actualidad de Referencias (fechas/versiones)",
                "pregunta": "¬øSe indica la vigencia o fecha de publicaci√≥n de los recursos, y se facilita al usuario la posibilidad de buscar asesor√≠a m√°s especializada?"
            }
        },
        "pesos_sub": [0.1, 0.9, 0.5, 0.5, 0.7]  # Suma: 2.7
    },

    "C5": {
        "nombre": "Claridad Temporal y de Recursos",
        "peso": 0.3,  # Bajo
        "subcriterios": {
            "C5.1": {
                "desc": "Indicar Estimaciones Temporales",
                "pregunta": "¬øLa respuesta proporciona estimaciones de tiempo para llevar a cabo las acciones?"
            },
            "C5.2": {
                "desc": "Proporcionar Informaci√≥n sobre Costos Aproximados",
                "pregunta": "¬øIncluye referencias a costos aproximados o rangos de inversi√≥n necesarios?"
            },
            "C5.3": {
                "desc": "Se√±alar Recursos Adicionales Necesarios",
                "pregunta": "¬øSe identifican los recursos humanos o tecnol√≥gicos imprescindibles para la implementaci√≥n?"
            },
            "C5.4": {
                "desc": "Especificar Plazos Legales o Fechas L√≠mite",
                "pregunta": "¬øSe mencionan posibles plazos o requerimientos formales vinculados a la normativa (si aplica)?"
            },
            "C5.5": {
                "desc": "Sugerir Mecanismos de Seguimiento y Control",
                "pregunta": "¬øSe recomiendan m√©todos de seguimiento, auditor√≠as o apoyo de profesionales externos para proyectos m√°s complejos?"
            }
        },
        "pesos_sub": [0.3, 0.1, 0.5, 0.9, 0.7]  # Suma: 2.5
    }
}

print("‚úÖ RECAVA Quality Protocol cargado (25 subcriterios con PESOS DEL DOCUMENTO).")
print("\nPesos de Criterios Principales (normalizados):")
for c_key, config in CRITERIOS_RECAVA.items():
    print(f"  {c_key}: {config['peso']:.4f} ({config['peso']*100:.2f}%)")
print(f"\nSuma total: {sum(CRITERIOS_RECAVA[k]['peso'] for k in ['C1','C2','C3','C4','C5']):.4f}")

# @title Mathematical Engine (Sugeno Lambda Measure - V15 Corrected)
import numpy as np
from scipy.optimize import fsolve
import itertools

def calcular_lambda(pesos):
    def ecuacion(l):
        return np.prod(1 + l * pesos) - (1 + l)
    suma_pesos = np.sum(pesos)
    if abs(suma_pesos - 1.0) < 1e-9: return 0.0
    inicial = -0.5 if suma_pesos > 1 else 1.0
    try: return fsolve(ecuacion, inicial)[0]
    except: return 0.0

def medida_sugeno(indices_conjunto, pesos, l_valor):
    pesos_np = np.array(pesos)
    if len(indices_conjunto) == 0: return 0.0
    idx = list(indices_conjunto)
    if abs(l_valor) < 1e-9: return np.sum(pesos_np[idx])
    producto = np.prod(1 + l_valor * pesos_np[idx])
    return (producto - 1) / l_valor

def generate_custom_fuzzy_measure(weights_list):
    pesos = np.array(weights_list)
    l = calcular_lambda(pesos)
    elementos = [f"s{i}" for i in range(len(weights_list))]
    fuzzy_measure = {}
    for n in range(1, len(elementos) + 1):
        for comb in itertools.combinations(elementos, n):
            indices = [int(e[1:]) for e in comb]
            val = medida_sugeno(indices, pesos, l)
            fuzzy_measure[tuple(sorted(comb))] = val
    return fuzzy_measure, l

def robust_score_parser(data):
    """Normaliza notas: 0.85 -> 8.5 | 85 -> 8.5 | 8.5 -> 8.5"""
    try:
        val = float(data.get('nota', 0))
        if val <= 1.0: val = val * 10
        elif val > 10: val = val / 10
        return min(max(val, 0.0), 10.0), data.get('comentario', 'N/A')
    except: return 0.0, "FAIL: Formato inv√°lido"

def calculate_choquet_generic(scores_list, weights_list, return_details=False):
    """Agregaci√≥n de Choquet. Devuelve valor normalizado 0-1 y opcionalmente detalles."""
    f = np.array([s / 10.0 for s in scores_list]) # Entrada 0-10 -> Interno 0-1
    measure, l_value = generate_custom_fuzzy_measure(weights_list)
    labels = [f"s{i}" for i in range(len(f))]
    x_dict = dict(zip(labels, f))
    sorted_x = sorted(x_dict.items(), key=lambda item: item[1])
    cvx, f_prev = 0, 0

    calc_steps = []
    for i in range(len(sorted_x)):
        remaining = tuple(sorted([sorted_x[j][0] for j in range(i, len(sorted_x))]))
        vHi = measure.get(remaining, 0)
        delta = sorted_x[i][1] - f_prev
        cvx += delta * vHi
        calc_steps.append({
            'step': i+1,
            'criterion': sorted_x[i][0],
            'value': sorted_x[i][1],
            'delta': delta,
            'measure': vHi,
            'contribution': delta * vHi
        })
        f_prev = sorted_x[i][1]

    if return_details:
        return float(cvx), {
            'lambda': l_value,
            'measure': {str(k): v for k, v in measure.items()},
            'steps': calc_steps
        }
    return float(cvx)

print("‚úÖ Mathematical engine (Choquet + Sugeno) ready.")

# @title Funci√≥n para generar feedback detallado
def generar_feedback_detallado(iter_results, detailed_scores, final_score_pct):
    """
    Genera el feedback en el formato solicitado con puntuaciones por criterio.

    Args:
        iter_results: Dict con las puntuaciones agregadas de cada criterio (C1-C5)
        detailed_scores: Dict con las puntuaciones de cada subcriterio
        final_score_pct: Puntuaci√≥n total final

    Returns:
        str: Feedback formateado
    """
    # Obtener nombres de criterios
    criterios_nombres = {
        "C1": CRITERIOS_RECAVA["C1"]["nombre"],
        "C2": CRITERIOS_RECAVA["C2"]["nombre"],
        "C3": CRITERIOS_RECAVA["C3"]["nombre"],
        "C4": CRITERIOS_RECAVA["C4"]["nombre"],
        "C5": CRITERIOS_RECAVA["C5"]["nombre"]
    }

    # Construir el feedback
    criterios_feedback = ", ".join([
        f"{criterios_nombres[c_key]}: {iter_results[c_key]:.1f}"
        for c_key in ["C1", "C2", "C3", "C4", "C5"]
    ])

    feedback = f"Retroalimentaci√≥n: Puntuaci√≥n: {final_score_pct:.1f}. La puntuaci√≥n que has conseguido en los siguientes criterios es: {criterios_feedback}"

    return feedback

# @title Main Experiment Execution: Agentic RLMF V15 (OPTIMIZADO)
import json
import re
import pandas as pd
from tabulate import tabulate
from concurrent.futures import ThreadPoolExecutor, as_completed

# ‚ö° OPTIMIZACI√ìN: Funci√≥n para auditor√≠a paralela
def audit_single_model(m_id, sys_instr, current_response, client_local, robust_score_parser):
    """
    Audita con un solo modelo. Dise√±ada para ejecuci√≥n paralela.
    Returns: dict con resultado para thread-safe processing.
    """
    try:
        response = client_local.chat.completions.create(
            model=m_id,
            messages=[
                {"role": "system", "content": sys_instr},
                {"role": "user", "content": f"RESPUESTA:\n{current_response}"}
            ],
            temperature=0.3  # Aumentada de 0.1 a 0.3 para m√°s variabilidad y cr√≠tica
        )

        if response is None:
            raise ValueError("Respuesta None")
        if not hasattr(response, 'choices') or len(response.choices) == 0:
            raise ValueError("Sin choices")
        if not response.choices[0].message or not response.choices[0].message.content:
            raise ValueError("Mensaje None")

        raw_out = response.choices[0].message.content
        json_match = re.search(r"\{.*\}", raw_out, re.DOTALL)

        if json_match:
            data = json.loads(json_match.group())
            score, comment = robust_score_parser(data)
            return {'success': True, 'model': m_id, 'score': score, 'comment': comment}
        else:
            return {'success': False, 'model': m_id, 'score': 0.0, 'comment': 'N/A', 'error': 'No JSON'}
    except Exception as e:
        return {'success': False, 'model': m_id, 'score': 0.0, 'comment': 'N/A', 'error': str(e)[:80]}


def run_recava_agentic_audit(user_query, local_models_list, success_threshold=90.0):
    """Ejecuta el ciclo completo de auditor√≠a con Agentic intelligence."""

    # Recuperaci√≥n de contexto de la base de datos vectorial
    # ‚ö° Usar versi√≥n cacheada
    retrieved_context = hybrid_search_engine_cached(user_query)

    initial_response = ""
    best_overall_response = ""
    best_overall_score = -1.0
    history_log = []
    best_score_history = []
    iterations_without_improvement = 0  # Contador de paciencia
    patience = 5  # Iteraciones sin mejora antes de parar (configurable)
    min_improvement_threshold = 0.5  # Mejora m√≠nima significativa en % (configurable)
    feedback_traces = []
    best_iteration_matrix = {}
    best_math_details = {}  # CORRECCI√ìN: Inicializar como dict
    all_iterations_data = []  # Para almacenar datos de todas las iteraciones
    all_responses_by_iteration = []  # Para guardar la respuesta de cada iteraci√≥n

    # --- SETUP INICIAL POR PANTALLA ---
    print(f"\n{'='*100}")
    print(f"üöÄ CONFIGURACI√ìN DEL EXPERIMENTO RECAVA-AI")
    print(f"{'='*100}")
    print(f"üìå PREGUNTA: {user_query}")
    print(f"ü§ñ MODELOS AUDITORES: {', '.join(local_models_list)}")
    print(f"üìä CRITERIOS DE EVALUACI√ìN: 5 bloques, 25 subcriterios")
    print(f"‚öôÔ∏è M√âTODO: Doble Integraci√≥n de Choquet (Sugeno Lambda)")
    print(f"üéØ UMBRAL DE √âXITO: {success_threshold}%")
    print(f"üõë PARADA ANTICIPADA: Mejora < 10% en 3 iteraciones consecutivas")
    print(f"‚è±Ô∏è M√ÅXIMO DE ITERACIONES: 25")
    print(f"{'='*100}\n")

    # PHASE 0: Initial generation
    print(f"üöÄ [PHASE 0] GENERANDO RESPUESTA INICIAL (GPT-4o)...")
    initial_sys = MAIN_MODEL_BEHAVIOR.format(feedback_placeholder="") + f"\nContexto: {retrieved_context}"
    initial_response = client_cloud.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": initial_sys}, {"role": "user", "content": user_query}]
    ).choices[0].message.content

    current_response = initial_response
    print(f"\n{'='*100}")
    print(f"üìÑ RESPUESTA INICIAL COMPLETA:")
    print(f"{'='*100}")
    print(initial_response)
    print(f"{'='*100}\n")

    # Ciclo de refinamiento (M√°ximo 25 iteraciones)
    for iteration in range(25):
        iter_id = iteration + 1
        print(f"\n{'‚ñà'*100}")
        print(f"ITERACI√ìN {iter_id}/25 - AUDITOR√çA ENJAMBRE")
        print(f"{'‚ñà'*100}")

        vote_matrix = {f"C{i}": [[] for _ in range(5)] for i in range(1, 6)}
        detailed_scores = {}

        # --- PHASE 1: ATOMIC EVALUATION ---
        for c_idx in range(1, 6):
            c_key = f"C{c_idx}"
            config = CRITERIOS_RECAVA[c_key]
            detailed_scores[c_key] = {}
            print(f"\nüì° Bloque {c_key}: {config['nombre']}")

            for s_idx, (sub_code, sub_data) in enumerate(config['subcriterios'].items()):
                sub_desc = sub_data['desc']
                sub_pregunta = sub_data['pregunta']
                print(f"   üîé Sub {sub_code}: {sub_desc}")

                # ‚ö° OPTIMIZACI√ìN: Auditor√≠as en PARALELO
                # Los auditores ahora responden a una pregunta espec√≠fica
                sys_instr = f"{AUDITOR_BEHAVIOR}\nSUB-CRITERIO: {sub_desc}\nPREGUNTA A EVALUAR: {sub_pregunta}\n{JSON_CONTRACT}"

                with ThreadPoolExecutor(max_workers=3) as executor:
                    futures = {
                        executor.submit(audit_single_model, m_id, sys_instr, current_response, client_local, robust_score_parser): m_id
                        for m_id in local_models_list
                    }

                    for future in as_completed(futures):
                        result = future.result()

                        if result['success']:
                            print(f"      ü§ñ [{result['model'][:20]}...] -> Nota: {result['score']:.1f} | {result['comment'][:50]}...")
                            feedback_traces.append({
                                "Iteration": iter_id, "Criterion": c_key, "Subcriterion": sub_code,
                                "Model": result['model'], "Score": result['score'], "Comment": result['comment']
                            })
                            vote_matrix[c_key][s_idx].append(result['score'])
                        else:
                            print(f"      ‚ùå [{result['model'][:20]}...] -> ERROR: {result.get('error', 'Unknown')}")

                # Calcular media del subcriterio
                if vote_matrix[c_key][s_idx]:
                    s_mean = sum(vote_matrix[c_key][s_idx])/len(vote_matrix[c_key][s_idx])
                else:
                    s_mean = 0.0
                detailed_scores[c_key][sub_code] = round(s_mean, 2)
                print(f"      üìä MEDIA SUB-CRITERIO {sub_code}: {s_mean:.2f}\n")

        # --- PHASE 2: DOUBLE CHOQUET AGGREGATION ---
        iter_results = {}
        block_scores = []
        block_weights = [CRITERIOS_RECAVA[f"C{i}"]['peso'] for i in range(1, 6)]
        block_lambdas = []
        block_measures = {}

        print(f"\n‚öñÔ∏è AGREGACI√ìN DE CHOQUET (SUGENO LAMBDA):")
        for c_key in ["C1", "C2", "C3", "C4", "C5"]:
            sub_scores = list(detailed_scores[c_key].values())
            sub_weights = CRITERIOS_RECAVA[c_key]["pesos_sub"]
            c_val, block_details = calculate_choquet_generic(sub_scores, sub_weights, return_details=True)
            iter_results[c_key] = c_val * 10
            block_scores.append(iter_results[c_key])
            block_lambdas.append(block_details['lambda'])
            block_measures[c_key] = block_details
            print(f"   - {c_key}: {iter_results[c_key]:.2f}/10 (Œª={block_details['lambda']:.4f})")

        # Agregaci√≥n final
        final_choquet_val, final_details = calculate_choquet_generic(block_scores, block_weights, return_details=True)
        final_score_pct = final_choquet_val * 100
        print(f"\n‚≠ê PUNTUACI√ìN TOTAL ITERACI√ìN {iter_id}: {final_score_pct:.2f}% (Œª_final={final_details['lambda']:.4f})")

        # Guardar historial
        iter_summary = {**iter_results, "Total": final_score_pct}
        history_log.append(iter_summary)

        # Guardar datos completos de la iteraci√≥n
        iteration_data = {
            'iteration': iter_id,
            'scores': iter_results,
            'subscores': detailed_scores,
            'total': final_score_pct
        }
        all_iterations_data.append(iteration_data)

        # Guardar la respuesta de esta iteraci√≥n
        all_responses_by_iteration.append({
            'iteration': iter_id,
            'response': current_response,
            'score': final_score_pct
        })

        # Actualizar mejor resultado
        if final_score_pct > best_overall_score:
            best_overall_score = final_score_pct
            best_overall_response = current_response
            best_iteration_matrix = detailed_scores
            best_math_details = {  # CORRECCI√ìN: Guardar detalles matem√°ticos
                'Lambdas': {
                    'Block_Lambdas': {f'C{i+1}': block_lambdas[i] for i in range(5)},
                    'Final_Lambda': final_details['lambda']
                },
                'Measures': {
                    **{k: v['measure'] for k, v in block_measures.items()},
                    'FINAL': final_details['measure']
                },
                'Steps': final_details['steps']
            }
            print("üåü NUEVO R√âCORD DE CALIDAD REGISTRADO.")

        # Imprimir tabla de evoluci√≥n hasta ahora
        print(f"\nüìä EVOLUCI√ìN DE PUNTUACIONES (Iteraciones 1-{iter_id}):")
        df_history = pd.DataFrame(history_log)
        df_history.insert(0, 'Iter', range(1, len(df_history)+1))
        print(tabulate(df_history, headers='keys', tablefmt='grid', floatfmt='.2f', showindex=False))

        # --- PARADA ANTICIPADA: M√âTODO DE PACIENCIA ---
        best_score_history.append(best_overall_score)

        if len(best_score_history) >= 2:
            recent_improvement = best_score_history[-1] - best_score_history[-2]

            if recent_improvement > min_improvement_threshold:
                iterations_without_improvement = 0
                print(f"‚úÖ Mejora detectada: +{recent_improvement:.2f}%")
            else:
                iterations_without_improvement += 1
                print(f"‚è≥ Sin mejora significativa ({iterations_without_improvement}/{patience} intentos)")

            if iterations_without_improvement >= patience:
                total_improvement = best_overall_score - best_score_history[0]
                print(f"\nüõë PARADA ANTICIPADA: {patience} iteraciones sin mejora > {min_improvement_threshold}%")
                print(f"   Mejora total acumulada: {total_improvement:.2f}%")
                break

        if best_overall_score >= success_threshold:
            print(f"\nüéâ UMBRAL DE √âXITO ALCANZADO: {best_overall_score:.2f}% >= {success_threshold}%")
            break

        # --- PHASE 3: BLIND NUMERIC REFINEMENT CON FEEDBACK DETALLADO ---
        if iteration < 24:
            # MODIFICACI√ìN: Generar feedback detallado con nombres de criterios
            feedback_detallado = generar_feedback_detallado(iter_results, detailed_scores, final_score_pct)

            print(f"\nüìù Feedback generado para el modelo:")
            print(f"   {feedback_detallado}")

            refine_sys = MAIN_MODEL_BEHAVIOR.format(feedback_placeholder=feedback_detallado) + f"\nContexto: {retrieved_context}"
            refine_msg = f"""PREGUNTA ORIGINAL DEL USUARIO:
{user_query}

TU RESPUESTA ANTERIOR:
{current_response}

INSTRUCCIONES:
Mejora tu respuesta anterior bas√°ndote en el feedback recibido en el system prompt.
C√©ntrate especialmente en los criterios con puntuaciones m√°s bajas.
Mant√©n lo que est√© bien y corrige/ampl√≠a lo que necesite mejora.
"""

            print(f"\nüîÑ Refinando respuesta con GPT-4o...")
            current_response = client_cloud.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "system", "content": refine_sys}, {"role": "user", "content": refine_msg}]
            ).choices[0].message.content

    # --- RESPUESTA FINAL COMPLETA ---
    print(f"\n{'='*100}")
    print(f"üìÑ RESPUESTA FINAL COMPLETA (Mejor Iteraci√≥n):")
    print(f"{'='*100}")
    print(best_overall_response)
    print(f"{'='*100}\n")

    # --- TABLA RESUMEN DE LA PREGUNTA ---
    print(f"\n{'='*100}")
    print(f"üìä RESUMEN DE RESULTADOS PARA ESTA PREGUNTA")
    print(f"{'='*100}")

    first_score = history_log[0]['Total'] if history_log else 0.0
    print(f"Pregunta: {user_query}")
    print(f"Iteraciones ejecutadas: {len(history_log)}")
    print(f"Puntuaci√≥n Inicial: {first_score:.2f}%")
    print(f"Mejor Puntuaci√≥n: {best_overall_score:.2f}%")
    print(f"Mejora Absoluta: {best_overall_score - first_score:.2f}%")
    print(f"Mejora Relativa: {((best_overall_score - first_score) / first_score * 100) if first_score > 0 else 0:.2f}%")

    print(f"\nüìà EVOLUCI√ìN DE PUNTUACIONES POR ITERACI√ìN:")
    df_evolution = pd.DataFrame(history_log)
    df_evolution.insert(0, 'Iter', range(1, len(df_evolution)+1))
    print(tabulate(df_evolution, headers='keys', tablefmt='grid', floatfmt='.2f', showindex=False))

    # Tabla de subcriterios de la mejor iteraci√≥n
    if best_iteration_matrix:
        print(f"\nüìã PUNTUACIONES DETALLADAS (Mejor Iteraci√≥n):")
        flat_rows = []
        for c_key in ['C1', 'C2', 'C3', 'C4', 'C5']:
            if c_key in best_iteration_matrix:
                for s_key, val in best_iteration_matrix[c_key].items():
                    flat_rows.append({
                        "Bloque": c_key,
                        "Subcriterio": s_key,
                        "Descripci√≥n": CRITERIOS_RECAVA[c_key]['subcriterios'][s_key]['desc'][:40] + "...",
                        "Nota": val
                    })
        df_subscores = pd.DataFrame(flat_rows)
        print(tabulate(df_subscores, headers='keys', tablefmt='grid', floatfmt='.2f', showindex=False))

    print(f"{'='*100}\n")

    # --- GENERACI√ìN DE INFORME FINAL ---
    valid_flag = "S√ç" if best_overall_score >= success_threshold else "NO"

    # Preparar matriz de subcriterios para el informe
    if best_iteration_matrix:
        flat_rows = []
        for c_key, subs in best_iteration_matrix.items():
            for s_key, val in subs.items():
                flat_rows.append({
                    "Bloque": c_key,
                    "Subcriterio": s_key,
                    "Nota": val
                })
        df_clean = pd.DataFrame(flat_rows)
        matrix_markdown = tabulate(df_clean, headers='keys', tablefmt='grid', floatfmt='.2f', showindex=False)
    else:
        matrix_markdown = "N/A"

    # CORRECCI√ìN: Verificar que best_math_details existe antes de usarlo
    if best_math_details:
        lambdas_info = json.dumps(best_math_details['Lambdas'], indent=2, ensure_ascii=False)
        measures_info = json.dumps(best_math_details['Measures']['FINAL'], indent=2, ensure_ascii=False)
    else:
        lambdas_info = "No disponible"
        measures_info = "No disponible"

    report = f"""
    {"="*100}
    üìú INFORME FINAL DEL EXPERIMENTO RECAVA-AI
    {"="*100}
    1. DESCRIPCI√ìN: Auditor√≠a h√≠brida con Agentic Intelligence y refinamiento ciego mediante Doble Choquet.
    2. M√âTRICAS:
      - Puntuaci√≥n Inicial: {first_score:.2f}%
      - Mejor Puntuaci√≥n: {best_overall_score:.2f}%
      - Mejora Absoluta: {best_overall_score - first_score:.2f}%

    3. TRAZAS DE ITERACIONES:
    {tabulate(pd.DataFrame(history_log), headers='keys', tablefmt='grid', floatfmt='.2f')}

    4. RETROALIMENTACI√ìN (Matriz Mejor Iteraci√≥n):
    {matrix_markdown}

    5. AN√ÅLISIS MATEM√ÅTICO DETALLADO (Sugeno Lambda):
    - Par√°metros Lambda por bloque:
    {lambdas_info}
    - Ejemplo de Medidas Difusas aplicadas (Bloque Final):
    {measures_info}

    6. CIERRE DEL EXPERIMENTO:
    - PREGUNTA: {user_query}
    - RESPUESTA INICIAL (primeros 200 chars): {initial_response[:200]}...
    - RESPUESTA FINAL (primeros 200 chars): {best_overall_response[:200]}...
    - ¬øV√ÅLIDA?: {valid_flag}
    {"="*100}
    """

    return initial_response, best_overall_response, best_overall_score, pd.DataFrame(history_log), report, pd.DataFrame(feedback_traces), all_iterations_data, all_responses_by_iteration

print("‚úÖ Main execution function ready.")

# @title Execute Experiment & Save Logs (CORREGIDO)
import sys
import os
import shutil
from datetime import datetime
import json

# --- 1. CLASE TEE PARA LOGGING DUAL ---
class Tee(object):
    def __init__(self, *files):
        self.files = files
    def write(self, obj):
        for f in self.files:
            f.write(obj)
            f.flush()  # escritura en tiempo real
    def flush(self):
        for f in self.files:
            f.flush()

# üíæ FUNCI√ìN PARA GUARDAR LOG INDIVIDUAL POR PREGUNTA
def save_query_log(query_idx, query_text, initial_text, final_text, report, summary_df, feedback_df, iterations_data, all_responses, path_recava):
    """
    Guarda el log completo de una pregunta individual.
    Formato: QXX_YYYYMMDD_HHMMSS_query_snippet.txt
    """
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_query = query_text[:40].replace("/", "_").replace("?", "").replace("¬ø", "").replace(":", "")
        filename = f"Q{query_idx:02d}_{timestamp}_{safe_query}.txt"
        filepath = os.path.join(path_recava, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write("="*100 + "\n")
            f.write(f"RECAV-AI - LOG DE PREGUNTA {query_idx}\n")
            f.write("="*100 + "\n")
            f.write(f"Timestamp: {timestamp}\n")
            f.write(f"Pregunta: {query_text}\n")
            f.write("="*100 + "\n\n")

            # Respuesta inicial
            if initial_text:
                f.write("RESPUESTA INICIAL (Iteraci√≥n 0):\n")
                f.write("-"*100 + "\n")
                f.write(initial_text + "\n")
                f.write("-"*100 + "\n\n")

            # Todas las respuestas por iteraci√≥n
            if all_responses:
                f.write("="*100 + "\n")
                f.write("EVOLUCI√ìN DE RESPUESTAS POR ITERACI√ìN\n")
                f.write("="*100 + "\n\n")

                for resp_data in all_responses:
                    f.write(f"{'‚îÄ'*100}\n")
                    f.write(f"ITERACI√ìN {resp_data['iteration']} - Puntuaci√≥n: {resp_data['score']:.2f}%\n")
                    f.write(f"{'‚îÄ'*100}\n")
                    f.write(resp_data['response'] + "\n")
                    f.write(f"{'‚îÄ'*100}\n\n")

            # Respuesta final
            if final_text:
                f.write("="*100 + "\n")
                f.write("RESPUESTA FINAL (Mejor Iteraci√≥n):\n")
                f.write("="*100 + "\n")
                f.write(final_text + "\n")
                f.write("="*100 + "\n\n")

            # Reporte de evoluci√≥n
            if report:
                f.write(report + "\n\n")

            # Tabla de evoluci√≥n
            if summary_df is not None and len(summary_df) > 0:
                f.write("="*100 + "\n")
                f.write("EVOLUCI√ìN DE PUNTUACIONES POR ITERACI√ìN\n")
                f.write("="*100 + "\n")
                f.write(summary_df.to_string(index=False) + "\n\n")

            # Datos de iteraciones (JSON)
            if iterations_data:
                f.write("="*100 + "\n")
                f.write("DATOS DETALLADOS DE ITERACIONES (JSON)\n")
                f.write("="*100 + "\n")
                f.write(json.dumps(iterations_data, indent=2, ensure_ascii=False) + "\n\n")

            # Feedback de auditores
            if feedback_df is not None and len(feedback_df) > 0:
                f.write("="*100 + "\n")
                f.write(f"FEEDBACK DE AUDITORES (Total: {len(feedback_df)} registros)\n")
                f.write("="*100 + "\n")
                f.write(feedback_df.head(100).to_string(index=False) + "\n")

        print(f"   üíæ Log guardado: {filename}")
        return filepath

    except Exception as e:
        print(f"   ‚ö†Ô∏è Error guardando log: {e}")
        return None

# --- 2. CONFIGURACI√ìN DEL EXPERIMENTO ---
TEST_QUERY_LIST = [
    #"¬øQu√© consecuencias podr√≠a tener que un proveedor no cumpla la CSDDD en materia de trabajo forzoso?",
    #"¬øQu√© iniciativas de econom√≠a circular promueve la UE y c√≥mo puedo aplicarlas en la fabricaci√≥n de electr√≥nicos?",
    #"¬øQu√© beneficios ofrece el EMAS (Eco-Management and Audit Scheme) en el contexto del Pacto Verde Europeo?",
    "¬øQu√© documentos debo solicitar a un proveedor de madera para cumplir con la regulaci√≥n de deforestaci√≥n en la UE?",
    "¬øC√≥mo eval√∫o si las condiciones de trabajo de un subcontratista cumplen los requisitos de la CSDDD?",
    "¬øQu√© normativa debo cumplir para vender mis productos org√°nicos dentro de la UE?",
    "¬øPodr√≠as explicar c√≥mo funciona el Convenio de Basilea para el control de desechos peligrosos y su relaci√≥n con la UE?",
    "¬øQu√© requisitos impone la UE para la importaci√≥n de pescado con criterios de pesca sostenible?",
    #"¬øC√≥mo se realiza un mapa de riesgos de derechos humanos en la cadena de suministro para cumplir la CSDDD?",
    #"¬øPodr√≠as darme un ejemplo de cuestionario de auditor√≠a social para proveedores de calzado, alineado con la CSDDD?",
    "¬øQu√© obligaciones legales tengo en la UE al usar sustancias qu√≠micas restringidas (REACH)?",
    "¬øCu√°les son los principales sellos de comercio justo que se reconocen en la UE para caf√© y cacao?",
    "¬øQu√© herramientas inform√°ticas existen para rastrear el origen de las materias primas y cumplir la regulaci√≥n sobre deforestaci√≥n?",
    "¬øPodr√≠as explicarme qu√© es una Declaraci√≥n de Performance Medioambiental de Producto (EPD) y su rol en la UE?",
    "¬øC√≥mo creo un sistema de gesti√≥n de quejas o reclamaciones relacionado con proveedores para cumplir la CSDDD?",
    #"¬øQu√© son los par√°metros ESG (ambientales, sociales y de gobernanza) y c√≥mo integrarlos en mi informe CSRD?",
    "¬øC√≥mo manejar la transici√≥n energ√©tica en una empresa que se dedica al transporte a√©reo para alinearse con la UE?",
    "Mi empresa produce embalajes. ¬øC√≥mo puedo introducir criterios de ecodise√±o siguiendo las directivas europeas?",
    #"¬øQu√© pasos debo seguir para analizar la huella de pl√°stico en mi l√≠nea de productos alimentarios seg√∫n la UE?",
    "¬øC√≥mo se aplica la Directiva de Informes de Sostenibilidad Corporativa (CSRD) a las grandes empresas en la UE y qu√© cambios introduce?",
    #"¬øCu√°les son los riesgos de no respetar la legislaci√≥n local sobre igualdad de g√©nero en la contrataci√≥n, seg√∫n la CSRD?",
    "¬øQu√© pasos se requieren para que mi empresa logre una certificaci√≥n de comercio justo que sea reconocida en la UE?",
    "Mi organizaci√≥n busca la neutralidad en la huella de carbono. ¬øQu√© directrices europeas y marcos GRI recomiendas seguir?",
    "¬øPodr√≠as detallar qu√© es la propuesta de Directiva de Debida Diligencia alemana y c√≥mo se alinea con la CSDDD?",
    #"¬øC√≥mo reducir las emisiones de gases de efecto invernadero en el sector agr√≠cola siguiendo la normativa europea?"
]

# --- 3. EJECUCI√ìN DEL EXPERIMENTO ---
print("\n" + "="*100)
print("üöÄ INICIANDO EXPERIMENTO RECAVA-AI - OPTIMIZADO CON FEEDBACK DETALLADO")
print("="*100)
print(f"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"üìã Preguntas a procesar: {len(TEST_QUERY_LIST)}")
print("="*100 + "\n")

# Guardar stdout original
original_stdout = sys.stdout

# Crear archivo de log global
log_filename = f"experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
log_file = open(log_filename, "w", encoding="utf-8")

# Redirigir stdout a ambos (consola y archivo)
sys.stdout = Tee(sys.stdout, log_file)

# Variables para tracking global
all_results = []
experiment_errors = []

try:
    # ITERAR SOBRE CADA PREGUNTA
    for i, TEST_QUERY in enumerate(TEST_QUERY_LIST, start=1):
        print(f"\n{'#'*100}")
        print(f"PROCESANDO PREGUNTA {i}/{len(TEST_QUERY_LIST)}")
        print(f"{'#'*100}")

        try:
            # Ejecutar experimento para esta pregunta
            initial_text, final_text, final_score, summary_df, report, feedback_df, iterations_data, all_responses = run_recava_agentic_audit(
                user_query=TEST_QUERY,
                local_models_list=LOCAL_MODELS,
                success_threshold=90.0
            )

            # Guardar log individual
            save_query_log(i, TEST_QUERY, initial_text, final_text, report, summary_df, feedback_df, iterations_data, all_responses, PATH_RECAVA)

            # Guardar resultado en lista global
            all_results.append({
                'query_index': i,
                'query': TEST_QUERY,
                'final_score': final_score,
                'iterations': len(summary_df),
                'summary_df': summary_df,
                'iterations_data': iterations_data
            })

            print(f"\n‚úÖ PREGUNTA {i} COMPLETADA EXITOSAMENTE")
            print(f"   Score Final: {final_score:.2f}%")
            print(f"   Iteraciones: {len(summary_df)}")

        except KeyboardInterrupt:
            print(f"\n‚ö†Ô∏è EXPERIMENTO INTERRUMPIDO POR EL USUARIO EN LA PREGUNTA {i}")
            experiment_errors.append({
                'query_index': i,
                'query': TEST_QUERY,
                'error_type': 'KeyboardInterrupt',
                'message': 'Experimento interrumpido por usuario'
            })
            continue

        except json.JSONDecodeError as e:
            error_msg = f"‚ùå ERROR DE PARSING JSON en pregunta {i}: {str(e)}"
            print(f"\n{error_msg}")
            import traceback
            print(traceback.format_exc())
            experiment_errors.append({
                'query_index': i,
                'query': TEST_QUERY,
                'error_type': 'JSONDecodeError',
                'message': str(e)
            })
            continue

        except NameError as e:
            error_msg = f"‚ùå ERROR DE VARIABLE NO DEFINIDA en pregunta {i}: {str(e)}"
            print(f"\n{error_msg}")
            import traceback
            print(traceback.format_exc())
            experiment_errors.append({
                'query_index': i,
                'query': TEST_QUERY,
                'error_type': 'NameError',
                'message': str(e)
            })
            continue

        except Exception as e:
            error_msg = f"‚ùå ERROR CR√çTICO DURANTE EL EXPERIMENTO (QUERY {i}: {TEST_QUERY}): {str(e)}"
            print(f"\n{error_msg}")
            import traceback
            print(traceback.format_exc())
            experiment_errors.append({
                'query_index': i,
                'query': TEST_QUERY,
                'error_type': type(e).__name__,
                'message': str(e)
            })
            continue

    # --- RESUMEN FINAL DEL EXPERIMENTO COMPLETO ---
    print("\n" + "="*100)
    print("üìä RESUMEN FINAL DEL EXPERIMENTO COMPLETO")
    print("="*100)
    print(f"üìÖ Finalizado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìà Total de preguntas procesadas: {len(TEST_QUERY_LIST)}")
    print(f"‚úÖ Preguntas exitosas: {len(TEST_QUERY_LIST) - len(experiment_errors)}")
    print(f"‚ùå Preguntas con errores: {len(experiment_errors)}")

    if experiment_errors:
        print("\n‚ö†Ô∏è ERRORES DETECTADOS:")
        for err in experiment_errors:
            print(f"  ‚Ä¢ Query {err['query_index']}: {err['error_type']} - {err['message'][:100]}")

    # Tabla resumen de resultados por pregunta
    if all_results:
        print("\nüìä TABLA DE EVOLUCI√ìN POR PREGUNTA:")
        print("="*100)

        summary_rows = []
        for res in all_results:
            if res['summary_df'] is not None and len(res['summary_df']) > 0:
                first_row = res['summary_df'].iloc[0]
                last_row = res['summary_df'].iloc[-1]

                summary_rows.append({
                    'Q': res['query_index'],
                    'Pregunta': res['query'][:50] + '...',
                    'Iters': res['iterations'],
                    'Score_Inicial': first_row['Total'],
                    'Score_Final': res['final_score'],
                    'Mejora': res['final_score'] - first_row['Total'],
                    'C1_Ini': first_row.get('C1', 0),
                    'C1_Fin': last_row.get('C1', 0),
                    'C2_Ini': first_row.get('C2', 0),
                    'C2_Fin': last_row.get('C2', 0),
                    'C3_Ini': first_row.get('C3', 0),
                    'C3_Fin': last_row.get('C3', 0),
                    'C4_Ini': first_row.get('C4', 0),
                    'C4_Fin': last_row.get('C4', 0),
                    'C5_Ini': first_row.get('C5', 0),
                    'C5_Fin': last_row.get('C5', 0)
                })

        if summary_rows:
            df_summary = pd.DataFrame(summary_rows)
            print(tabulate(df_summary, headers='keys', tablefmt='grid', floatfmt='.2f', showindex=False))

            # Tabla detallada de subcriterios por pregunta
            print("\nüìã EVOLUCI√ìN DETALLADA DE SUBCRITERIOS POR PREGUNTA:")
            print("="*100)

            for res in all_results:
                print(f"\nüîç Pregunta {res['query_index']}: {res['query'][:60]}...")
                print("-"*100)

                if res['iterations_data'] and len(res['iterations_data']) > 0:
                    first_iter = res['iterations_data'][0]
                    last_iter = res['iterations_data'][-1]

                    subscore_rows = []
                    for c_key in ['C1', 'C2', 'C3', 'C4', 'C5']:
                        if c_key in first_iter['subscores'] and c_key in last_iter['subscores']:
                            for sub_code in first_iter['subscores'][c_key].keys():
                                subscore_rows.append({
                                    'Criterio': c_key,
                                    'Subcriterio': sub_code,
                                    'Score_Inicial': first_iter['subscores'][c_key][sub_code],
                                    'Score_Final': last_iter['subscores'][c_key][sub_code],
                                    'Mejora': last_iter['subscores'][c_key][sub_code] - first_iter['subscores'][c_key][sub_code]
                                })

                    if subscore_rows:
                        df_subscores = pd.DataFrame(subscore_rows)
                        print(tabulate(df_subscores, headers='keys', tablefmt='grid', floatfmt='.2f', showindex=False))
                        print()

            # Estad√≠sticas agregadas
            print("\nüìà ESTAD√çSTICAS AGREGADAS:")
            print("="*100)
            avg_initial = sum([r['Score_Inicial'] for r in summary_rows]) / len(summary_rows)
            avg_final = sum([r['Score_Final'] for r in summary_rows]) / len(summary_rows)
            avg_improvement = sum([r['Mejora'] for r in summary_rows]) / len(summary_rows)
            avg_iters = sum([r['Iters'] for r in summary_rows]) / len(summary_rows)

            print(f"  ‚Ä¢ Puntuaci√≥n Inicial Promedio: {avg_initial:.2f}%")
            print(f"  ‚Ä¢ Puntuaci√≥n Final Promedio: {avg_final:.2f}%")
            print(f"  ‚Ä¢ Mejora Promedio: {avg_improvement:.2f}%")
            print(f"  ‚Ä¢ Iteraciones Promedio: {avg_iters:.1f}")
            print(f"  ‚Ä¢ Mejora Relativa Promedio: {(avg_improvement / avg_initial * 100) if avg_initial > 0 else 0:.2f}%")

    print("\n" + "="*100)
    print("‚úÖ EXPERIMENTO FINALIZADO EXITOSAMENTE")
    print("="*100 + "\n")

except Exception as e:
    print(f"\n‚ùå ERROR FATAL EN EL EXPERIMENTO: {str(e)}")
    import traceback
    print(traceback.format_exc())

finally:
    # Restauramos siempre stdout al final de TODO el experimento
    sys.stdout = original_stdout

# --- 4. PERSISTENCIA EN GOOGLE DRIVE ---
try:
    if not os.path.exists(PATH_RECAVA):
        os.makedirs(PATH_RECAVA)

    destination_path = os.path.join(PATH_RECAVA, log_filename)
    shutil.move(log_filename, destination_path)
    print(f"\n‚úÖ Experimento finalizado. Log guardado en Drive: {destination_path}")

    # Guardar resumen de errores si existen
    if experiment_errors:
        error_log_path = os.path.join(
            PATH_RECAVA,
            f"errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(error_log_path, 'w', encoding='utf-8') as f:
            json.dump(experiment_errors, f, indent=2, ensure_ascii=False)
        print(f"‚ö†Ô∏è Log de errores guardado en: {error_log_path}")

    # Guardar resultados consolidados
    if all_results:
        results_path = os.path.join(
            PATH_RECAVA,
            f"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        # Convertir DataFrames a dict para serializaci√≥n JSON
        serializable_results = []
        for res in all_results:
            serializable_results.append({
                'query_index': res['query_index'],
                'query': res['query'],
                'final_score': res['final_score'],
                'iterations': res['iterations'],
                'summary': res['summary_df'].to_dict('records') if res['summary_df'] is not None else []
            })

        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        print(f"üìä Resultados consolidados guardados en: {results_path}")

except Exception as e:
    print(f"\n‚ö†Ô∏è El experimento termin√≥ pero no se pudo mover el log a Drive: {e}")
    print(f"El archivo local es: {log_filename}")