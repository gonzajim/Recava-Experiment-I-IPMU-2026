# -*- coding: utf-8 -*-
"""Experimento I - RECAV-AI Fase II Paper IPMU - 2026.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pyEatiAv76gE56OhZt7IUWKXK3bUC84c

# RECAV-AI Phase II: Hybrid Swarm Audit & Choquet Refinement
This notebook implements the second phase of the RECAV-AI experiment. It uses a Hybrid Swarm Intelligence architecture where a Cloud LLM (Generator) is audited by a local swarm of models (Auditors) through an atomic evaluation process.

**1. Environment Setup and Dependencies**

First, we mount Google Drive to ensure persistent storage for our logs and retrieve the necessary API keys using Colab's userdata.
"""

from google.colab import drive
drive.mount('/content/drive')

# Install required libraries
!pip install openai pinecone sentence-transformers pandas tabulate -q

# @title Mount Google Drive and Install Dependencies
from google.colab import drive, userdata
import os
import shutil
import sys
from datetime import datetime

# Mount Google Drive
drive.mount('/content/drive')

# Configuration of paths
PATH_RECAVA = "/content/drive/MyDrive/RECAVA-AI-STARTUP/Experimentos"
os.makedirs(PATH_RECAVA, exist_ok=True)

print("âœ… Environment ready.")

"""**2. Knowledge Base Connectivity (Pinecone & Embeddings)**

We connect to the existing Pinecone index to enable Hybrid Search (Vectors + Graph). This provides the context for the Generator model.
"""

# @title Knowledge Base Integration
from pinecone import Pinecone
from sentence_transformers import SentenceTransformer

# Retrieve API Keys securely
PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')
OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')

# Initialize Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)
# Replace with your specific index host
index = pc.Index(host="https://uclm-corpus-roma-dptaw1c.svc.aped-4627-b74a.pinecone.io")

# Initialize Embeddings model
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

def hybrid_search_engine(query, top_k=20):
    """Retrieves context from Pinecone injecting Graph triplets."""
    query_vector = embed_model.encode(query).tolist()
    results = index.query(vector=query_vector, top_k=top_k, include_metadata=True)

    context = ""
    for match in results['matches']:
        meta = match['metadata']
        context += f"\n[SOURCE: {meta.get('source', 'Unknown')}]\n"
        context += f"GRAPH RELATIONS: {meta.get('triplets', 'N/A')}\n"
        context += f"CONTENT: {meta.get('text', '')}\n"
        context += "-"*40 + "\n"
    return context

print("âœ… Knowledge Base online.")

"""**3. Local Server Connection (LM Studio via Ngrok)**

This block establishes the connection between Colab and your local inference server (LM Studio). We define the specific models that will act as Adversary Agents.

"""

# @title Local Inference Server Configuration
import openai

# Replace with your actual Ngrok URL
URL_NGROK = "https://prodigally-nonaddicting-deborah.ngrok-free.dev/v1"

client_local = openai.OpenAI(base_url=URL_NGROK, api_key="lm-studio")
client_cloud = openai.OpenAI(api_key=OPENAI_API_KEY)

# Define the models to be used in the swarm
LOCAL_MODELS = [
    "mistral_7b_0-3_oh-dcft-v3.1-claude-3-5-sonnet-20241022",
    "google/gemma-3-12b",
    "deepseek/deepseek-r1-distill-llama-8b"
]

print("âœ… Models configured.")

"""**4. Experiment Parameters and Behavioral Logic**

We define the RECAVA Protocol and the behavior for both the Generator and the Auditors. We use specific system instructions in Spanish to maintain linguistic consistency in the legal domain.
"""

# @title Behavioral Configuration & Rationale

# Generator Behavior
MAIN_MODEL_BEHAVIOR = """
  Eres un experto senior en sostenibilidad, diligencia debida y cumplimiento normativo.
  Tu objetivo es generar una respuesta de mÃ¡xima excelencia tÃ©cnica.
  Usa lenguaje del dominio legal; cita artÃ­culos y estÃ¡ndares especÃ­ficos (ISO, ESRS, GRI).
"""

# Auditor Behavior (Refined for Atomic Evaluation)
AUDITOR_BEHAVIOR = """
  Eres un experto senior en sostenibilidad, diligencia debida y cumplimiento normativo.
  Tu objetivo es evaluar la respuesta de otro modelo basÃ¡ndote en un subcriterio especÃ­fico que recibirÃ¡s.
  Proporciona ÃšNICAMENTE aspectos de mejora.
  Si el subcriterio se cumple perfectamente, indica "Sin mejoras necesarias".
  Proporciona un grado de cumplimiento del subcriterio que estÃ¡s evaluando del 0 al 1 en base a la pregunta correpsondiente de ese criterio. Siendo 0 que no se cumple y 1 que se cumple perfectamente.
"""

# Atomic JSON Contract (Single sub-criterion evaluation)
JSON_CONTRACT = """
  REGLA DE SALIDA OBLIGATORIA:
  1. Tu respuesta debe ser EXCLUSIVAMENTE un objeto JSON vÃ¡lido.
  2. NO incluyas procesos de pensamiento (Chain of Thought), ni etiquetas <think>.
  3. El JSON debe tener esta estructura exacta:
  {
    "nota": valor entre 0 y 1,
    "comentario": "str"
  }
  donde 'nota' es un entero entre 0 y 1 y 'comentario' detalla los aspectos de mejora.
"""

# Dictionary as the Single Source of Truth
CRITERIOS_RECAVA = {
    "C1": {
        "nombre": "ESTRUCTURA CLARA Y JERARQUIZADA",
        "peso": 0.2,
        "subcriterios": {
            "1.1": "Secciones: Â¿Usa encabezados y subtÃ­tulos claros?",
            "1.2": "ProgresiÃ³n: Â¿Va de lo general a lo especÃ­fico de forma lÃ³gica?",
            "1.3": "PriorizaciÃ³n: Â¿Destaca las ideas mÃ¡s importantes para el usuario?",
            "1.4": "Elementos Visuales: Â¿Emplea listas, viÃ±etas o negritas eficazmente?",
            "1.5": "Ejemplos: Â¿Incluye casos concretos para aclarar conceptos?"
        }
    },
    "C2": {
        "nombre": "PRECISIÃ“N Y ADECUACIÃ“N AL CONTEXTO",
        "peso": 0.3,
        "subcriterios": {
            "2.1": "Ajuste al tema: Â¿Responde exactamente a lo preguntado?",
            "2.2": "Exactitud Normativa: Â¿Es fiel a la normativa citada sin mezclar regulaciones?",
            "2.3": "Relevancia: Â¿Evita informaciÃ³n genÃ©rica o irrelevante?",
            "2.4": "Escenarios: Â¿Es aplicable a distintos tamaÃ±os de empresa o sectores?",
            "2.5": "Profundidad TÃ©cnica: Â¿El lenguaje se adapta al perfil del usuario?"
        }
    },
    "C3": {
        "nombre": "ENFOQUE PRÃCTICO Y TOMA DE DECISIONES",
        "peso": 0.3,
        "subcriterios": {
            "3.1": "Sugerencias: Â¿Ofrece recomendaciones concretas para actuar?",
            "3.2": "Pros y Contras: Â¿Menciona ventajas y desventajas de las opciones?",
            "3.3": "ImplementaciÃ³n: Â¿Describe una secuencia de pasos lÃ³gica?",
            "3.4": "ObstÃ¡culos: Â¿Advierte sobre limitaciones o posibles barreras?",
            "3.5": "Ayuda Externa: Â¿Sugiere cuÃ¡ndo contactar con asesores especializados?"
        }
    },
    "C4": {
        "nombre": "REFERENCIAS Y RECURSOS ADICIONALES",
        "peso": 0.1,
        "subcriterios": {
            "4.1": "Fuentes: Â¿Cita organizaciones o marcos reconocidos (GRI, ISO, ONU)?",
            "4.2": "Enlaces: Â¿Proporciona links a guÃ­as, herramientas o portales oficiales?",
            "4.3": "EstÃ¡ndares: Â¿Menciona normativas internacionales pertinentes?",
            "4.4": "Manuales: Â¿Refiere a metodologÃ­as o guÃ­as tÃ©cnicas especÃ­ficas?",
            "4.5": "Vigencia: Â¿Indica la fecha o versiÃ³n de los recursos citados?"
        }
    },
    "C5": {
        "nombre": "CLARIDAD TEMPORAL Y DE RECURSOS",
        "peso": 0.1,
        "subcriterios": {
            "5.1": "Tiempos: Â¿Proporciona estimaciones de duraciÃ³n de los procesos?",
            "5.2": "Costos: Â¿Incluye rangos de inversiÃ³n o costos aproximados?",
            "5.3": "RRHH/TecnologÃ­a: Â¿Identifica personal o herramientas imprescindibles?",
            "5.4": "Plazos Legales: Â¿Menciona fechas lÃ­mite normativas si existen?",
            "5.5": "Seguimiento: Â¿Recomienda mÃ©todos de auditorÃ­a o control posterior?"
        }
    }
}

"""**5. Mathematical Engine (Sugeno Choquet Integral)**

The evaluative "heart" of this experiment implements a non-additive aggregation model using the Sugeno $\lambda$-measure to account for the interdependence between sub-criteria. Unlike simple averages, this model identifies if sub-criteria are synergistic ($\lambda > 0$) or redundant ($\lambda < 0$).

**A. Definition of the Sugeno Fuzzy Measure ($g_\lambda$)**

The Choquet integral requires a fuzzy measure $\mu$ assigned to every possible subset. We utilize the Sugeno framework to derive these weights automatically from individual densities:Densities: Each sub-criterion is assigned an initial weight (0.2 for a block of 5).$\lambda$ Parameter: The interaction parameter $\lambda$ is solved numerically such that $\lambda + 1 = \prod_{i=1}^n (1 + \lambda g_i)$. This parameter determines how sub-criteria overlap or amplify each other.Subset Weights: For any subset $A$, the measure is calculated as $g(A) = \frac{1}{\lambda} \left[ \prod_{i \in A} (1 + \lambda g_i) - 1 \right]$.

**B. Aggregation Algorithm**

The engine follows four logical steps:

1.   **Numerical Normalization:** Agent scores are scaled to the $[0, 1]$ range via robust_score_parser (correcting for 0-100 hallucinations).
2.   **Sorting:** Values are sorted in ascending order: $f_{(1)} \le f_{(2)} \le \dots \le f_{(n)}$.
3.   **Sugeno Measurement:** At each step $i$, the Sugeno measure $\mu$ is calculated for the set of criteria that remain to be satisfied.
4.   **Integral Calculation ($C_{\mu}$):**$$C_{\mu}(f) = \sum_{i=1}^{n} (f_{(i)} - f_{(i-1)}) \cdot \mu(A_{(i)})$$where $A_{(i)}$ is the subset of criteria $\{s_i, \dots, s_n\}$.

**C. Robustness Features**

The robust_score_parser acts as a data-cleaning layer, identifying if a local model responded with a raw percentage (e.g., 85) or a decimal (e.g., 8.5), performing a true proportional normalization to the $[0, 1]$ interval before it reaches the Choquet engine.'''

"""

# @title Mathematical Engine (Sugeno Lambda Measure)
import numpy as np
from scipy.optimize import fsolve

def robust_score_parser(data):
    """
    Normaliza correctamente las puntuaciones al rango [0, 1].
    Maneja escalas 0-1, 0-10 y alucinaciones 0-100.
    """
    try:
        raw_val = data.get('nota', 0)
        val = float(raw_val)

        # LÃ“GICA DE NORMALIZACIÃ“N REPARADA:
        if val > 10:
            # Si el agente usa escala 0-100 (ej. 85), normalizamos a 0.85
            val = val / 100
        elif val > 1.0:
            # Si el agente usa escala 0-10 (ej. 8.5), normalizamos a 0.85
            val = val / 10
        # Si val <= 1.0, asumimos que ya estÃ¡ en el rango [0, 1] solicitado

        # Cap de seguridad final para asegurar el rango [0, 1]
        val = min(max(val, 0.0), 1.0)
        return val, data.get('comentario', 'N/A')
    except:
        return 0.0, "FAIL: Error de formato de nota"

def calcular_lambda(pesos):
    """Encuentra el valor de lambda para la medida de Sugeno."""
    def ecuacion(l):
        return np.prod(1 + l * pesos) - (1 + l)

    suma_pesos = np.sum(pesos)
    if abs(suma_pesos - 1.0) < 1e-9:
        return 0.0

    inicial = -0.5 if suma_pesos > 1 else 1.0
    return fsolve(ecuacion, inicial)[0]

def medida_sugeno(indices_conjunto, pesos, l_valor):
    """Calcula la medida mu para un subconjunto de criterios mediante Sugeno."""
    pesos_np = np.array(pesos)
    if len(indices_conjunto) == 0:
        return 0.0

    idx = list(indices_conjunto)
    if abs(l_valor) < 1e-9:
        return np.sum(pesos_np[idx])

    pesos_seleccionados = pesos_np[idx]
    producto = np.prod(1 + l_valor * pesos_seleccionados)
    return (producto - 1) / l_valor

def calculate_choquet_score(sub_means):
    """
    FunciÃ³n de compatibilidad con el ciclo principal RECAVA.
    Calcula la Integral de Choquet usando la medida de Sugeno.
    """
    notas = np.array(sub_means)
    # Asumimos pesos equitativos (0.2 cada uno) para los 5 subcriterios del bloque
    pesos_individuales = np.array([0.2, 0.2, 0.2, 0.2, 0.2])

    # 1. Calcular parÃ¡metro Lambda de Sugeno
    l = calcular_lambda(pesos_individuales)

    # 2. Ordenar notas e Ã­ndices para Choquet
    indices_ordenados = np.argsort(notas)
    notas_ordenadas = notas[indices_ordenados]

    valor_choquet = 0
    nota_anterior = 0
    n = len(notas)

    for i in range(n):
        diferencia_nota = notas_ordenadas[i] - nota_anterior
        # Conjunto de criterios que sobreviven (nota >= actual)
        criterios_sobrevivientes = indices_ordenados[i:]

        # 3. Calcular Medida Difusa (mu) dinÃ¡micamente
        mu = medida_sugeno(criterios_sobrevivientes, pesos_individuales, l)

        valor_choquet += diferencia_nota * mu
        nota_anterior = notas_ordenadas[i]

    return float(valor_choquet)

"""**6. Main Experiment Execution**

This block runs the iterative refinement loop. It scrubbs CoT reasoning and processes strictly structured JSON feedback.
"""

# @title Main Experiment Execution: Swarm RLMF V14 (Blind Numeric Refinement)
import json, re, pandas as pd

def run_recava_agentic_audit(user_query, local_models_list, success_threshold=90.0):

    # Context retrieval from VECTORIAL DB in PINECONE
    retrieved_context = hybrid_search_engine(user_query)

    best_overall_response = ""
    best_overall_score = -1.0
    history_log = []

    # PHASE 0: Initial generation
    print(f"\n{'='*100}\n------- EXPERIMENT DATA ---------------------\n{'='*100}")
    print(f"\n{'='*100}\n Question: {user_query}\n")
    print(f"\n{'='*100}\nðŸš€ [PHASE 0] INITIAL GENERATION\n{'='*100}")
    initial_sys = MAIN_MODEL_BEHAVIOR.format(nota_final_pct=0.0) + f"\nContexto: {retrieved_context}"
    current_response = client_cloud.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": initial_sys}, {"role": "user", "content": user_query}]
    ).choices[0].message.content

    print(f"\n{'='*100}\n Response: {current_response}\n")

    for iteration in range(5):
        iter_id = iteration + 1
        print(f"\n{'â–ˆ'*100}\nITERATION {iter_id} - AUDIT\n{'â–ˆ'*100}")

        vote_matrix = {f"C{i}": [[] for _ in range(5)] for i in range(1, 6)}
        iter_feedback_pool = [] # Mantenemos esto para trazabilidad en logs

        # --- PHASE 1: EVALUATION (Atomic) ---
        for c_idx in range(1, 6):
            c_key = f"C{c_idx}"
            config = CRITERIOS_RECAVA[c_key]
            print(f"ðŸ“¡ Auditing Block {c_key}: {config['nombre']}...")

            for s_idx, (sub_code, sub_desc) in enumerate(config['subcriterios'].items()):
                print(f"   ðŸ”Ž Sub {sub_code}...", end=" ")
                last_comment = ""

                for m_id in local_models_list:
                    sys_instr = f"{AUDITOR_BEHAVIOR}\nSUB-CRITERIO A EVALUAR: {sub_desc}\n{JSON_CONTRACT}"

                    try:
                        raw_out = client_local.chat.completions.create(
                            model=m_id,
                            messages=[{"role": "system", "content": sys_instr},
                                      {"role": "user", "content": f"RESPUESTA A EVALUAR:\n{current_response}"}],
                            temperature=0.1
                        ).choices[0].message.content

                        clean_json_txt = re.sub(r'<think>.*?</think>', '', raw_out, flags=re.DOTALL)
                        data = json.loads(re.search(r"\{.*\}", clean_json_txt, re.DOTALL).group())

                        score, comment = robust_score_parser(data)
                        vote_matrix[c_key][s_idx].append(score)

                        # Guardamos feedback cualitativo SOLO para trazabilidad en logs
                        iter_feedback_pool.append(f"[{sub_code}] {m_id}: {comment}")
                        last_comment = comment

                    except Exception as e:
                        vote_matrix[c_key][s_idx].append(0.0)
                        print(f"âŒ Error en {m_id}: {e}", end=" ")

                print(f"âœ… vote: {score} with feedback: {last_comment[:50]}...")

        # --- PHASE 2: AGGREGATION & SCORING (Motor Sugeno) ---
        iter_results = {}
        for c_key, lists in vote_matrix.items():
            sub_means = [sum(v)/len(v) if v else 0.0 for v in lists]
            iter_results[c_key] = calculate_choquet_score(sub_means)

        final_score_pct = sum(iter_results[k] * CRITERIOS_RECAVA[k]['peso'] for k in CRITERIOS_RECAVA) * 100
        print(f"\nâš–ï¸ [ITERATION {iter_id} RESULTS]\nâ­ TOTAL SCORE: {final_score_pct:.2f}%")

        history_log.append({**iter_results, "Total": final_score_pct})

        if final_score_pct > best_overall_score:
            best_overall_score, best_overall_response = final_score_pct, current_response

        if best_overall_score >= success_threshold: break

        # --- PHASE 3: NUMERIC FEEDBACK & REFINEMENT (MODIFIED) ---
        if iteration < 4:
            # 1. ConstrucciÃ³n del feedback numÃ©rico en formato JSON solicitado
            numeric_report = []
            for k in CRITERIOS_RECAVA.keys():
                numeric_report.append({
                    "criterio": CRITERIOS_RECAVA[k]['nombre'],
                    "puntuaciÃ³n": round(iter_results[k], 4)
                })

            # Convertimos la lista de diccionarios a un string JSON
            json_feedback_str = json.dumps(numeric_report, ensure_ascii=False, indent=2)

            # 2. Trazabilidad: Imprimimos quÃ© se le envÃ­a al modelo (eliminada llamada al agregador)
            print(f"ðŸ“Š BLIND NUMERIC FEEDBACK FOR V{iter_id+1}:\n{json_feedback_str}\n")

            # 3. Refinamiento: El modelo principal recibe solo la nota y el desglose JSON
            refine_sys = MAIN_MODEL_BEHAVIOR.format(nota_final_pct=final_score_pct) + f"\nContexto: {retrieved_context}"

            refine_msg = (
                f"Tu versiÃ³n anterior obtuvo una puntuaciÃ³n de {final_score_pct:.2f}%. "
                f"Analiza los puntos bajos basÃ¡ndote Ãºnicamente en este desglose de criterios y puntuaciones (escala 0 a 1) "
                f"y genera una respuesta de mÃ¡xima excelencia tÃ©cnica:\n\n{json_feedback_str}"
            )

            current_response = client_cloud.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": refine_sys},
                    {"role": "user", "content": refine_msg}
                ]
            ).choices[0].message.content
            print(f"ðŸ”„ [REFINED RESPONSE V{iter_id+1} GENERATED VIA NUMERIC REINFORCEMENT]")

    return best_overall_response, best_overall_score, pd.DataFrame(history_log)

"""**7. Execution and Persistence**

Finally, we execute the test query, print the results, and move the logs to Google Drive for permanent storage.
"""

# @title Execute Experiment & Save Logs
import sys
import os
import shutil
from datetime import datetime

# --- 1. CLASE TEE PARA LOGGING DUAL ---
class Tee(object):
    def __init__(self, *files):
        self.files = files
    def write(self, obj):
        for f in self.files:
            f.write(obj)
            f.flush()  # escritura en tiempo real
    def flush(self):
        for f in self.files:
            f.flush()

# --- 2. CONFIGURACIÃ“N DEL EXPERIMENTO ---
TEST_QUERY_LIST = [
    "Â¿QuÃ© obligaciones introduce la Directiva CSRD para las empresas de la UE?",
    "Â¿QuÃ© requisitos trae la nueva regulaciÃ³n de la UE sobre productos asociados a la deforestaciÃ³n?",
    "Â¿PodrÃ­as explicarme quÃ© es el â€˜Greenwashingâ€™ y quÃ© sanciones podrÃ­a imponer la UE en un futuro?"
]
SUCCESS_THRESHOLD = 90.0
log_filename = f"Paper_IPMU_2026_experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

LOCAL_MODELS_RUN = [
    "google/gemma-3-12b",
    "deepseek/deepseek-r1-distill-llama-8b",
    "mistral_7b_0-3_oh-dcft-v3.1-claude-3-5-sonnet-20241022"
]

# --- 3. PROCESO DE EJECUCIÃ“N CON REDIRECCIÃ“N DE STDOUT ---
original_stdout = sys.stdout
try:
    with open(log_filename, "w", encoding="utf-8") as f:
        # Consola + fichero
        sys.stdout = Tee(original_stdout, f)

        print(f"ðŸš€ Iniciando Experimento del proyecto Recava para el Paper del IPMU: {datetime.now()}")
        print(f"ðŸŽ¯ SUCCESS_THRESHOLD: {SUCCESS_THRESHOLD}")
        print(f"ðŸ§  LOCAL_MODELS_RUN: {LOCAL_MODELS_RUN}")
        print("\n" + "="*80)

        for i, TEST_QUERY in enumerate(TEST_QUERY_LIST, start=1):
            try:
                print("\n" + "-"*80)
                print(f"ðŸ§ª PREGUNTA {i}/{len(TEST_QUERY_LIST)} â€” TEST_QUERY: {TEST_QUERY}")
                print("-"*80)

                final_text, final_score, summary_df = run_recava_agentic_audit(
                    TEST_QUERY,
                    LOCAL_MODELS_RUN,
                    success_threshold=SUCCESS_THRESHOLD
                )

                print("\n" + "="*60 + "\nðŸ“Š RESUMEN FINAL DE PUNTUACIONES\n" + "="*60)
                print(summary_df.to_markdown(index=False) if hasattr(summary_df, "to_markdown") else summary_df)

                print(f"\nðŸ† RESPUESTA GANADORA (Score: {final_score:.2f}%):")
                print(f"{'-'*60}\n{final_text}\n{'-'*60}")

            except Exception as e:
                print(f"\nâŒ ERROR CRÃTICO DURANTE EL EXPERIMENTO (QUERY: {TEST_QUERY}): {str(e)}")
                import traceback
                print(traceback.format_exc())

finally:
    # Restauramos siempre stdout al final de TODO el experimento
    sys.stdout = original_stdout

# --- 4. PERSISTENCIA EN GOOGLE DRIVE ---
try:
    if not os.path.exists(PATH_RECAVA):
        os.makedirs(PATH_RECAVA)

    destination_path = os.path.join(PATH_RECAVA, log_filename)
    shutil.move(log_filename, destination_path)
    print(f"\nâœ… Experimento finalizado. Log guardado en Drive: {destination_path}")
except Exception as e:
    print(f"\nâš ï¸ El experimento terminÃ³ pero no se pudo mover el log a Drive: {e}")
    print(f"El archivo local es: {log_filename}")